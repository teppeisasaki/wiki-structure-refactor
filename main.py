import os
from openai import AzureOpenAI

import dotenv

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã‚€ï¼ˆ.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”¨æ„ã—ã¦ãŠãã¨ä¾¿åˆ©ï¼‰
dotenv.load_dotenv()
api_base = os.getenv("AZURE_OPENAI_ENDPOINT")
deployment_name = os.getenv("AZURE_OPENAI_DEPLOYMENT")  # ä¾‹: gpt-4
client = AzureOpenAI(
    api_key=os.getenv("AZURE_OPENAI_KEY"), api_version="2023-07-01-preview"
)


# --- ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°ã‚’è¿½åŠ  ---
def calculate_token_count(text):
    # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—ã™ã‚‹ç°¡æ˜“çš„ãªæ–¹æ³•
    return len(text.split())


# --- ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã¨æ¦‚è¦ã‚’å–å¾—ã™ã‚‹é–¢æ•°ã‚’è¿½åŠ  ---
def get_file_list_and_summaries(directory):
    file_summaries = []
    for root, _, files in os.walk(directory):
        # wikiç›´ä¸‹ã®attachmentsãƒ•ã‚©ãƒ«ãƒ€ã¨.gitãƒ•ã‚©ãƒ«ãƒ€ã‚’ç„¡è¦–
        if root == os.path.join(directory, "attachments") or root == os.path.join(
            directory, ".git"
        ):
            continue
        for file in files:
            file_path = os.path.join(root, file)
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read()
                    summary = content[:200]  # ãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€åˆã®200æ–‡å­—ã‚’æ¦‚è¦ã¨ã—ã¦ä½¿ç”¨
                    file_summaries.append((file_path, summary))
            except (UnicodeDecodeError, FileNotFoundError) as e:
                print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {file_path} - {e}")
    return file_summaries


# --- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰ã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ã‚’å¤‰æ›´ ---
def build_prompt_from_files(file_summaries):
    file_details = "\n".join(
        [f"- {path}: {summary}" for path, summary in file_summaries]
    )
    base_prompt = f"""
ä»¥ä¸‹ã¯ç¾åœ¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã¨ãã‚Œãã‚Œã®æ¦‚è¦ã§ã™ã€‚

{file_details}

æœŸå¾…ã™ã‚‹å‡ºåŠ›:
1. æ–°ã—ã„ãƒ•ã‚©ãƒ«ãƒ€æ§‹æˆ
2. ç°¡å˜ãªèª¬æ˜
3. ãƒ•ã‚¡ã‚¤ãƒ«ç§»å‹•ææ¡ˆ
"""

    # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—
    token_count = calculate_token_count(base_prompt)
    print(f"ğŸ”¢ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {token_count}")

    return base_prompt


# --- ãƒ‡ãƒãƒƒã‚°ç”¨ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ› ---
def save_prompt_to_file(prompt, file_path="debug_prompt.txt"):
    with open(file_path, "w", encoding="utf-8") as f:
        f.write(prompt)
    print(f"âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ {file_path} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")


# --- LLM å‘¼ã³å‡ºã—ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æœ€å¤§åŒ– ---
def call_openai(prompt):
    max_tokens_for_response = 4096 - calculate_token_count(
        prompt
    )  # æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’å¼•ã
    response = client.chat.completions.create(
        model=deployment_name,
        messages=[
            {"role": "system", "content": "ã‚ãªãŸã¯æƒ…å ±è¨­è¨ˆã®å°‚é–€å®¶ã§ã™ã€‚"},
            {"role": "user", "content": prompt},
        ],
        temperature=0.7,
        max_tokens=max_tokens_for_response,  # å‹•çš„ã«è¨ˆç®—ã—ãŸãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨­å®š
        stream=True,  # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚’æœ‰åŠ¹åŒ–
    )

    result = ""
    print("ğŸ”„ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ä¸­...")
    for chunk in response:
        if hasattr(chunk, "choices") and chunk.choices:
            delta = chunk.choices[0].delta
            if hasattr(delta, "content"):
                content = delta.content
                if content is not None:
                    print(content, end="", flush=True)  # é€”ä¸­çµæœã‚’è¡¨ç¤º
                    result += content

    print("\nâœ… ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Œäº†")
    return result


wiki_root = "./wiki"

# --- ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã¨æ¦‚è¦ã‚’å–å¾— ---
file_summaries = get_file_list_and_summaries(wiki_root)

# --- æ–°ã—ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰ ---
prompt = build_prompt_from_files(file_summaries)

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä¿å­˜
save_prompt_to_file(prompt)

# --- LLM å‘¼ã³å‡ºã— ---
llm_response = call_openai(prompt)

# --- çµæœã‚’ä¿å­˜ ---
with open("wiki_restructure_suggestion.txt", "w", encoding="utf-8") as f:
    f.write(llm_response)

print("âœ… ææ¡ˆæ§‹æˆã‚’ wiki_restructure_suggestion.txt ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
